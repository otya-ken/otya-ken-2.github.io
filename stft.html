<!DOCTYPE html>
<html lang="ja">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Web Audio API</title>
    </head>
    <body>
        <h1>音源の作成　ファイルをbufferとして読み込む</h1>
        <button id="play">play</button>
        <button id="stop">stop</button>
        <canvas id="canvas"></canvas>
    </body>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.0/dist/tf.min.js"></script>
    <script>
        window.AudioContext = window.AudioContext || window.webkitAudioContext;
        const ctx = new AudioContext();
    
        let sampleSource;
        // 再生中でtrue
        let isPlaying = false;

        const wav_file = "./BASIC5000_0001.wav";
        // 音源を取得しAudioBuffer形式に変換して返す関数
        async function setupSample() {
            // const response = await fetch("../BASIC5000_0001.wav");
            const response = await fetch(wav_file);
            const arrayBuffer = await response.arrayBuffer();
            // Web Audio APIで使える形式に変換
            const audioBuffer = await ctx.decodeAudioData(arrayBuffer);
            return audioBuffer;
        }
        // AudioBufferをctxに接続し再生する関数
        function playSample(ctx, audioBuffer) {
            sampleSource = ctx.createBufferSource();
            // 変換されたバッファーを音源として設定
            sampleSource.buffer = audioBuffer;
            // 出力につなげる
            sampleSource.connect(ctx.destination);
            sampleSource.start();
            isPlaying = true
        }
        
        async function stft_draw_istft() {
            var frame_size = 25;
            var frame_shift = 10;
            const analyser = ctx.createAnalyser();
            const source = ctx.createBufferSource();
            const file = await fetch(wav_file);
            const arraybuffer = await file.arrayBuffer();
            const audiobuffer = await ctx.decodeAudioData(arraybuffer);
            const sample_frequency = audiobuffer.sampleRate;
            const num_samples = sample_frequency * audiobuffer.duration;
            console.log(audiobuffer);
            
            var buffer = audiobuffer.getChannelData(0);
            console.log(buffer.slice(0, 5));
            const signals = tf.tensor1d(buffer);
            
            console.log(sample_frequency);
            // console.log(num_samples === audiobuffer.length);   True
            console.log(num_samples);
            
            // stft
            frame_size = parseInt(sample_frequency * frame_size * 0.001);   // frame_size = 1102
            frame_shift = parseInt(sample_frequency * frame_shift * 0.001); // frame_shift = 441
            
            var fft_size = 1;
            while(fft_size < frame_size){
                fft_size *= 2;
            }
            analyser.fftsize = 1 << fft_size;         // fft_size = 2048
            var length = analyser.frequencyBinCount;  // length = 1024
            
            const num_frames = Math.trunc((num_samples - frame_size) / frame_shift) + 1;
            
            console.log(frame_size, frame_shift, fft_size);
            
            var spectrogram = tf.signal.stft(signals, frame_size, frame_shift, fft_size, 
                                             tf.signal.hamming_window);
            console.log(spectrogram);
            spectrogram.print();      // 317 x 1025行列(テンソル)
            
            var n_spectrogram = spectrogram.dataSync();   // 一次元配列324925要素
            console.log(n_spectrogram.slice(0, 5));
            
            // var spectrogram = tf.zeros([num_frames, parseInt(fft_size / 2) + 1]);
            
            // for (var frame_idx = 0; frame_idx < num_frames; frame_idx++) {
            //     var start_index = frame_idx * frame_shift;
            //     var frame = buffer.slice(start_index, start_index + frame_size);
            //     for (var n = 0; n < frame_size; n++) {
            //         frame[n] *= 0.54 - 0.46 * Math.cos(2 * Math.PI * n / (frame_size - 1))
            //     }
            //     
            // }
            
            // draw
            const canvas = document.getElementById('canvas');
            const graph = canvas.getContext('2d');
            for (let i = 0; i < 6; i++) {
                for (let j = 0; j < 6; j++) {
                    graph.fillStyle = `rgb(
                    ${Math.floor(255 - 42.5 * i)},
                    ${Math.floor(255 - 42.5 * j)},
                    0)`;
                graph.fillRect(j * 25, i * 25, 25, 25);
                }
            }
            
            // istft
            // var re_frequency = tf.signal.inverse_stft(spectrogram, frame_size, frame_shift, fft_size, 
            //                                          tf.signal.hamming_window);
            // re_frequency.print();
        }
        
        stft_draw_istft();
        
        document.querySelector("#play").addEventListener("click", async () => {
            // 再生中なら二重に再生されないようにする
            if(isPlaying) return;
            const sample = await setupSample();
            playSample(ctx, sample);
        });

        // oscillatorを破棄し再生を停止する
        document.querySelector("#stop").addEventListener("click", async () => {
            sampleSource?.stop();
            isPlaying = false
        });
        
    </script>
    <style>
        div{
            padding-bottom: 40px;
            border-bottom: 2px solid #666;
        }
    </style>
</html>
