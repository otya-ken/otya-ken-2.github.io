<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    
</head>
<body>
    <!--コメントアウト-->
    <p>練習</p>
    <!--<h3>48000Hz</h3>
    <button id="play1">play</button>
    <button id="stop1">stop</button>
    <h3>16000Hz</h3>
    <button id="play2">play</button>
    <button id="stop2">stop</button>-->
    <h3>raw data 8000Hz</h3>
    <button id="play3">play</button>
    <button id="stop3">stop</button>
    <!--<h3>cubic補完後</h3>
    <button id="play4">play</button>
    <button id="stop4">stop</button>-->
    <h3>predicted data 16000Hz</h3>
    <button id="play5">play</button>
    <button id="stop5">stop</button>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.0/dist/tf.min.js"></script>
    <script>
        window.AudioContext = window.AudioContext || window.webkitAudioContext;
        const ctx8000 = new AudioContext({"sampleRate":8000});
        const ctx16000 = new AudioContext({"sampleRate":16000});
        const analyser = ctx8000.createAnalyser();
        var frame_size = 1024;
        var fft_size = 1;
        while(fft_size < frame_size){
            fft_size *= 2;
        }
        //console.log(fft_size);
        analyser.fftSize = fft_size;         // fft_size = 2048
        var length = analyser.frequencyBinCount;  // length = 1024
        //console.log(analyser.fftSize);
        //console.log(length);


        let sampleSource;
        // 再生中でtrue
        let isPlaying = false;

        //const wav_file1 = "./BASIC5000_0001.wav";
        //const wav_file2 = "./BASIC5000_0001_3.wav";
        const wav_file3 = "./p225_001_6x.wav";
        // 音源を取得しAudioBuffer形式に変換して返す関数
        async function setupSample(ctx, wav) {
            // const response = await fetch("../BASIC5000_0001.wav");
            const response = await fetch(wav);
            const arrayBuffer = await response.arrayBuffer();
            //ctx.sampleRate = 8000;
            //console.log(ctx.sampleRate);
            // Web Audio APIで使える形式に変換
            const audioBuffer = await ctx.decodeAudioData(arrayBuffer);
            return audioBuffer;
        }

        function playSample(ctx, audioBuffer) {
            sampleSource = ctx.createBufferSource();
            // 変換されたバッファーを音源として設定
            sampleSource.buffer = audioBuffer;
            // 出力につなげる
            sampleSource.connect(ctx.destination);
            sampleSource.start();
            isPlaying = true
        }

        async function audio(){
            //const audiobuffer1 = await setupSample(wav_file1);
            //const audiobuffer2 = await setupSample(wav_file2);
            const audiobuffer3 = await setupSample(ctx8000, wav_file3);
            //const sample_frequency1 = audiobuffer1.sampleRate;
            //const sample_frequency2 = audiobuffer2.sampleRate;
            const sample_frequency3 = audiobuffer3.sampleRate;
            //console.log("元音源", sample_frequency1);
            //console.log("16000", sample_frequency2);
            //console.log("8000", sample_frequency3);
        }
        audio();

        //document.querySelector("#play1").addEventListener("click", async () => {
        //    // 再生中なら二重に再生されないようにする
        //    if(isPlaying) return;
        //    const sample = await setupSample(wav_file1);
        //    playSample(ctx, sample);
        //});

        //document.querySelector("#play2").addEventListener("click", async () => {
        //    // 再生中なら二重に再生されないようにする
        //    if(isPlaying) return;
        //    const re_audio = await setupSample(wav_file2);
        //    playSample(ctx, re_audio);
        //});

        document.querySelector("#play3").addEventListener("click", async () => {
            // 再生中なら二重に再生されないようにする
            if(isPlaying) return;
            const re_audio = await setupSample(ctx8000, wav_file3);
            playSample(ctx8000, re_audio);
        });

        // oscillatorを破棄し再生を停止する
        //document.querySelector("#stop1").addEventListener("click", async () => {
        //    sampleSource?.stop();
        //    isPlaying = false
        //});

        //document.querySelector("#stop2").addEventListener("click", async () => {
        //    sampleSource?.stop();
        //    isPlaying = false
        //});

        document.querySelector("#stop3").addEventListener("click", async () => {
            sampleSource?.stop();
            isPlaying = false
        });

        //async function model() {
        //    //const model = await tf.loadLayersModel('https://foo.bar/tfjs_artifacts/model.json');
        //    const model = await tf.loadLayersModel('./tfjs_target_dir/model.json');
        //    model.summary();
        //}
        //model()
        
        function upsampling2x(x) {
            // x : float32array
            // xt = tf.tensor1d(x);
            let l = x.length;
            let a = -0.75;
            let c1 = -0.125 * a + 0.5;
            c1 = tf.scalar(c1);
            let c2 = 0.125 * a;
            c2 = tf.scalar(c2);
    
            // 補完位置からみて左右に二つ隣までのサンプル値を使うので、左側に一つ、右側に二つパディング
            // また int16 のまま計算するとオーバーフローするのでいったん float32 に変換する
            //xx = np.concatenate([[x[0]], x.astype(np.float32), [x[-1], x[-1]]])
            let xx = new Float32Array(l+3);
            xx.set(x[0], 0);
            xx.set(x, 1);
            xx.set([x[x.length-1], x[x.length-1]], xx.length-2);
    
            // 補完値の計算
            //let y = c1 * (xx[1:-2] + xx[2:-1]) + c2 * (xx[:-3] + xx[3:])
            let xxtf = tf.tensor1d(xx);
            let xx1 = tf.add(tf.slice(xxtf, 1, xx.length-3), tf.slice(xxtf, 2, xx.length-3));
            let xx2 = tf.add(tf.slice(xxtf, 0, xx.length-3), tf.slice(xxtf, 3, xx.length-3));
            let y = tf.add(tf.mul(c1, xx1), tf.mul(c2, xx2));
            y = y.dataSync();
            // y : float32array
    
            // 補完した結果：[x0, y0, ..., x_{n-1}, y_{n-1}]]] を元の x の dtype に合わせて作成
            //let xy = np.ndarray(len(x) * 2, x.dtype)
            //let xy[::2] = x
            //let xy[1::2] = y
            let xy = new Float32Array(2*l);
            for (let i = 0; i < l; i++) {
                xy.set([x[i], y[i]], 2*i);
            }
            // xy : float32array
            return xy;
        }

        async function cubic2x(wav) {
            const audiobuffer = await setupSample(ctx8000, wav);
            const buffer = audiobuffer.getChannelData(0);
            const re_buffer = upsampling2x(buffer);
            return re_buffer;
        }
        //cubic2x(wav_file3).then(value => { return stft(cubic(value)) })
        //.then(value => {console.log(value)});
        
        //function cubic(wav_file4) {
        //    //const wav_file4 = cubic2x(wav_file3);
        //    //console.log(wav_file4.length);
        //    var re_audioBuffer = ctx16000.createBuffer(1, wav_file4.length, 16000);
        //    var re_spectrum_audio = re_audioBuffer.getChannelData(0);
        //    for (var i = 0; i < wav_file4.length; i++){
        //        re_spectrum_audio[i] = wav_file4[i];
        //    }
        //    document.querySelector("#play4").addEventListener("click", async () => {
        //        // 再生中なら二重に再生されないようにする
        //        if(isPlaying) return;
        //        playSample(ctx16000, re_audioBuffer);
        //    });
        //
        //    return re_spectrum_audio;
        //}
        
        async function stft(buffer) {
            //const audiobuffer = await setupSample(wav);
            //var buffer = audiobuffer.getChannelData(0);   //  一次元配列140679要素
            const signals = tf.tensor1d(buffer);

            //const property = await stft_property(wav);
            let num_frames = Math.trunc((buffer.length - 256) / 64) + 1;
            let property = [16000, 256, 64, 256, num_frames, buffer.length];

            var spectrogram = tf.signal.stft(signals, property[1], property[2], property[3], tf.signal.hammingWindow);

            // spectrogram : tensor2d
            return spectrogram;
        }

        async function t_istft(tensor) {
            // tensor : complex tensor2d
            const spectrogram = tensor;
            //const spectrogram = await stft(wav);
            //const property = await stft_property(wav);
            //const frame_size = property[1];
            //const frame_shift = property[2];
            //const fft_size = property[3];
            //const num_frames = property[4];
            //const t = fft_size/2 + 1;
            let num_frames = tensor.shape[0];
            let property = [16000, 256, 64, 256, num_frames, 0];

            // [350, 513] -> [350, 1024] 
            var r_spe = tf.real(spectrogram);
            var i_spe = tf.imag(spectrogram).mul(tf.scalar(-1));
            var rev_spe = tf.complex(r_spe.reverse(1).slice([0, 1], [num_frames, t-2]), i_spe.reverse(1).slice([0, 1], [num_frames, t-2]));
            var re_spectrogram = spectrogram.concat(rev_spe, 1);

            // .ifft()
            re_spectrogram = re_spectrogram.ifft();

            // ifft後の処理 real(実数のみ)かabs(絶対値)か
            var re_spectrogram = tf.real(re_spectrogram);

            // hamming窓を戻す
            // hamming = tf.scalar(0.54).sub(tf.scalar(0.46).mul(tf.cos((2*Math.PI)/(fft_size-1))))
            //re_spectrogram = re_spectrogram.div();
            var re_spectro = tf.tensor2d([[], ]);
            for (var n = 0; n < frame_size; n++) {
                var hamming = tf.scalar(0.54).sub(tf.scalar(0.46).mul(tf.cos((2*Math.PI*n)/fft_size)));
                re_spectro = tf.concat([re_spectro, re_spectrogram.slice([0, n], [num_frames, 1]).div(hamming)], 1);
            }
            console.log(re_spectro);
            re_spectro.print();

            // テンソル 350 x 1024
            // 400 x 349 + 1024    配列140624要素 ≠ 真のデータ 140679要素
            var data_spectrum = re_spectro.dataSync();
            //console.log(data_spectrum);
            var re_spectrum = new Float32Array(property[5]);
            for (var i = 0; i < num_frames-1; i++) {
                re_spectrum.set(data_spectrum.slice(i*frame_size, i*frame_size+frame_shift), i*frame_shift);
            }
            re_spectrum.set(data_spectrum.slice((num_frames-1)*frame_size, num_frames*frame_size), (num_frames-1)*frame_shift);
            //console.log(re_spectrum);

            //音源化　(不完全)
            var re_audioBuffer = ctx.createBuffer(1, re_spectrum.length, property[0]);
            var re_spectrum_audio = re_audioBuffer.getChannelData(0);
            for (var i = 0; i < re_spectrum.length; i++){
                re_spectrum_audio[i] = re_spectrum[i];
            }
            //console.log(re_spectrum_audio);
            //const re_audioBuffer = await ctx.decodeAudioData(re_spectrum);
            return re_audioBuffer;
        }

        function load_wav(spectrogram){
            const r_spe = tf.real(spectrogram);
            const i_spe = tf.imag(spectrogram);
            let magnitude = spectrogram.abs().log().add(10**(-10));
            let angle = tf.atan2(i_spe, r_spe);

            return [magnitude, angle];
        }

        function pred_spec_2d(time_size, sp_size, net, magnitude, angle) {
            const transpose = a => a[0].map((_, c) => a.map(r => r[c]));
            // time_size : 32
            // sp_size   : 129
            // net       : model
            // magnitude : 対数振幅スペクトル tensor
            // angle     : 位相スペクトル    tensor
            // return    : スペクトログラム 対数振幅スペクトル tensor, tensor
        
            // 処理開始時刻
            const time_start = performance.now();

            //ネスト化
            magnitude = magnitude.arraySync();
            angle = angle.arraySync();

            //出力の真ん中部分の採用
            //amp = 2 * np.sqrt(2)
            //time_size = 32
            //sp_size = 129
            let ur = 2;
            let data = magnitude;
            //console.log(data);

            //------
            // 入力バッチの作成
            //------
            // リストに集めておいて後で tf.Tensor に変換する
            //let input_sp = tf.tensor2d([[], ]);
            let input_sp = [];
        
            // half overlap しながら time_size 個のフレームのスペクトル下半分を集める
            //for i in range(0, len(data) - time_size + 1, time_size // 2):
            //    input_sp.append(data[i : i + time_size, : sp_size // ur])
        
            for (let i = 0; i < (data.length - time_size + 1) / Math.floor(time_size/2); i++) {
                i = i * Math.floor(time_size/2);
                //tf.concat([input_sp, data.slice([i, 0], [time_size, Math.floor(sp_size/ur)])], 1);
                input_sp = input_sp.concat(transpose(transpose(data.slice(i, i + time_size)).slice(0, Math.floor(sp_size/ur))));
            }
            // 全フレーム数が time_size // 2 で割り切れないときは
            // 最後に後ろから time_size フレーム取ってバッチに入れる
            //has_remainder = (len(data) % (time_size // 2) > 0)
            //if has_remainder:
            //    input_sp.append(data[-time_size :, : sp_size // ur])
            let has_remainder = (data.length % Math.floor(time_size/2) > 0);
            if (has_remainder) {
                input_sp = input_sp.concat(transpose(transpose(data.slice(data.length-time_size, data.length)).slice(0, Math.floor(sp_size/ur))));
            }
    
            // tf.Tensor に変換
            input_sp = tf.stack(input_sp);
    
            // 入力作成の終了時刻
            const time_preproc_finish = performance.now();
    
            //print("#frames", len(data))
            //print("input_sp.shape", input_sp.shape)
            //print("has_remainder", has_remainder)

            //------
            // 予測の実行
            //------
            // 入力の shape を (バッチサイズ, フレーム数, 周波数ビン数, 1) に変換する必要がある
            let output_sp = net.predict(tf.reshape(input_sp, [-1, time_size, Math.floor(sp_size/2), 1]));
    
            //print("output_sp.shape", output_sp.shape)
    
            // ネットワークの計算の終了時刻
            const time_net_finish = performance.now();

            //------
            // スペクトルの上半分を時間方向に貼り合わせる
            //------
            // リストに集めておいて後で tf.Tensor に変換する
            let upper_sp = []
            output_sp = output_sp.arraySync();
            //console.log(output_sp);

            // 最初の time_size / 4 フレーム
            //upper_sp.append(output_sp[0][:time_size // 4, :])
            upper_sp = upper_sp.concat(output_sp[0].slice(0, Math.floor(time_size/4)));
    
            // 各 time_size フレームの中央 2/4 を貼り合わせる
            let bgn = Math.floor(time_size / 4);     // time_size フレーム中での切り出し開始位置
            let end = Math.floor(time_size / 4 * 3); // time_size フレーム中での切り出し終了位置

            //for i in range(len(output_sp) - has_remainder):
            //    upper_sp.append(output_sp[i][bgn:end, :])
            for (let i = 0; i < output_sp.length - has_remainder; i++) {
                upper_sp = upper_sp.concat(output_sp[i].slice(bgn, end));
            }

            // (端数がない場合の）最後の time_size / 4 フレーム
            bgn = Math.floor(time_size / 4 * 3); // time_size フレーム中での切り出し開始位置
            end = time_size;                     // time_size フレーム中での切り出し終了位置
            let ix_in_batch = output_sp.length - 1 - has_remainder; // バッチ中での位置
            //console.log(output_sp.length);
            //console.log(has_remainder);
            //console.log(ix_in_batch);
            //upper_sp.append(output_sp[ix_in_batch][bgn:end, :])
            upper_sp = upper_sp.concat(output_sp[ix_in_batch].slice(bgn, end));

            // 端数がある場合
            if (has_remainder) {
                let n_remain = data.length % Math.floor(time_size / 2); // 余りのフレーム数
                //upper_sp.append(output_sp[-1][-n_remain:, :])
                upper_sp = upper_sp.concat(output_sp[output_sp.length-1].slice(output_sp[output_sp.length-1].length-n_remain, output_sp[output_sp.length-1].length));
            }
            
            //console.log(upper_sp);
            // 上半分のスペクトルを時間方向に繋げて Tensor にする
            upper_sp = tf.concat(upper_sp, axis=0);
            //print("upper_sp.shape", upper_sp.shape)
    
            //console.log(upper_sp);
            //console.log(data);
            //---------
            // 入力の下半分のスペクトルと貼り合わせる
            //---------
            //pred_sp = tf.concat([data[:, :sp_size // ur], upper_sp], axis=1)
            let pred_sp = tf.concat([transpose(data).slice(0, Math.floor(sp_size/ur)), upper_sp], axis=1);
            //print("pred_sp.shape", pred_sp.shape)
    
            // 後処理（データ整形）の終了時刻
            let time_postproc_finish = performance.now();

            //---------
            // ISFT を実行
            //---------
            // 入力の位相スペクトルと予測した振幅スペクトルから複素スペクトルを作る
            let complex_sp = tf.exp(tf.cast(pred_sp, tf.complex64) + angle * tf.complex(0.0, 1.0))
            // 小さいところをゼロにつぶす処理：不要？
            //spectral_cnn_2d = np.where(np.abs(spectral_cnn_2d) >= amp/10, spectral_cnn_2d, 0)
            // 逆短時間FFT
            //_, wav = signal.istft(complex_sp.numpy().T, 16000, nperseg=256, noverlap=64)
            //wav = np.array(wav, dtype=np.int16) // 戻しておかないとすごい音がする
            let wav = t_istft(complex_sp);
            //wav = wav.getChannelData(0);
    
            // ISTFT の終了時刻
            const time_istft_finish = performance.now();

            //---------
            // 時間計測結果を stderr に表示
            //---------
            console.log("Timing data:");
            console.log("  preproc :", time_preproc_finish - time_start);
            console.log("  network :", time_net_finish - time_preproc_finish);
            console.log("  postproc:", time_postproc_finish - time_net_finish);
            console.log("  istft   :", time_istft_finish - time_postproc_finish);
            console.log("-----------");
            console.log("  total   :", time_istft_finish - time_start);

            //---------
            // 波形と予測した振幅スペクトルを返す
            //---------
            return [wav, tf.transpose(pred_sp)];
        }

        async function pred_audio(){
            // 入力 元データ 8000Hz -> cubic2x 16000Hz -> stft -> 予測 -> istft -> 出力 16000Hz
            const model = await tf.loadLayersModel('./tfjs_target_dir/model.json');
            //const spectrogram = await cubic2x(wav_file3).then(value => { return stft(cubic(value)) });
            const cubic_wav = await cubic2x(wav_file3);
            const spectrogram = await stft(cubic_wav);
            const wav_data = load_wav(spectrogram);
            const magnitude = wav_data[0];
            const angle = wav_data[1];
            const predict= pred_spec_2d(time_size=32, sp_size=129, net=model, magnitude, angle);
            const pred_wav = predict[0];
            const pred_spec = predict[1];
            document.querySelector("#play5").addEventListener("click", async () => {
                // 再生中なら二重に再生されないようにする
                if(isPlaying) return;
                playSample(ctx16000, pred_wav);
            });
        }

        pred_audio();

        //document.querySelector("#stop4").addEventListener("click", async () => {
        //    sampleSource?.stop();
        //    isPlaying = false
        //});

        document.querySelector("#stop5").addEventListener("click", async () => {
            sampleSource?.stop();
            isPlaying = false
        });
    </script>
</body>
</html>
